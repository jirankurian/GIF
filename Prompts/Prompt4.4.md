**Your Current Task: Task 4.4 - Executing and Analyzing the Exoplanet Experiment**

**Protocol Reminder:** Before you begin, you must execute your full **Cognitive Cycle**. Review the `/Rules` directory, the `/docs` directory, the `/Reference/` for Phase 4, read all logs in the `.context/`, and analyze the existing codebase. You should assume that the `applications/poc_exoplanet/main_exo.py` script from the previous task can be run to produce detailed log files of an experiment. This task is about running that script and then making sense of its output. After your analysis, formulate your micro-plan for this task and present it for approval.

---

### **Task Objective**

Your goal is twofold:
1.  **Execute the Experiment:** Run the `main_exo.py` script to train and evaluate the GIF-DU framework on the exoplanet detection task.
2.  **Analyze and Report:** Create a dedicated analysis script that ingests the log files produced by the experiment, calculates all required performance and efficiency metrics, and generates the final, publication-quality tables and plots that validate the framework's capabilities, specifically mirroring Table IV from your research paper.

---

### **Domain & Technical Specifications**

#### **1. The Importance of Scientific Analysis and Reproducibility**

* **Domain Context:** In scientific research, running an experiment is only half the battle. The other half is analyzing the results in a rigorous, transparent, and reproducible way. The analysis code is just as important as the model code. We must create a clean, automated way to process our results so that anyone (including ourselves, months from now) can understand exactly how our final figures were generated.
* **Technical Approach:** We will create a dedicated analysis script or Jupyter Notebook. A **Jupyter Notebook** (`.ipynb`) is often preferred for this kind of work as it allows for a mix of code, visualizations, and explanatory text, creating a clear, narrative-driven analysis report.

#### **2. Implementation Details**

* **Action:** You will create a new file: `applications/poc_exoplanet/analysis.ipynb`.
* **Justification:** A dedicated notebook keeps the analysis logic separate from the training logic. It provides an interactive environment for exploring the data and producing high-quality plots, which is ideal for scientific reporting.
* **Required Libraries:** `polars` (for loading and manipulating the log data), `matplotlib` and `seaborn` (for creating high-quality plots), `scikit-learn` (for calculating metrics like F1-score and confusion matrices).

#### **3. The Analysis Notebook Structure**

The notebook should be structured with clear sections using Markdown headings.

* **Section 1: Setup and Data Loading**
    * **Action:** Import all necessary libraries. Define the path to the experiment log file(s) generated by `main_exo.py`. Use `polars.read_csv()` or `polars.read_json()` to load the results into a DataFrame.
    * **Display:** Show the first few rows of the DataFrame (`df.head()`) to confirm the data has loaded correctly.

* **Section 2: Training Process Visualization**
    * **Action:** Create a plot of the model's training performance over time.
    * **Implementation:**
        1.  Use `matplotlib.pyplot.plot()`.
        2.  The x-axis should be the training epoch or sample number.
        3.  The y-axis should show the training loss and validation accuracy over time.
        4.  The plot must have a title (e.g., "GIF-DU Training Performance on Exoplanet Task"), labeled axes, and a legend.
    * **Justification:** This plot is a crucial diagnostic tool. It shows us if the model is learning effectively, if it's overfitting, or if the training is unstable.

* **Section 3: Quantitative Performance Analysis (Generating Table IV)**
    * **Action:** Calculate the final performance metrics on the held-out test set.
    * **Implementation:**
        1.  From your results DataFrame, extract the ground truth labels and the model's final predictions for the test set.
        2.  Use `sklearn.metrics` to calculate:
            * `accuracy_score`
            * `recall_score` (This is the True Positive Rate or Sensitivity)
            * `precision_score`
            * `f1_score`
            * `confusion_matrix` (to derive the False Positive Rate).
        3.  Extract the neuromorphic performance stats (Total SynOps, Estimated Energy) from the logs.
        4.  **Crucially, you will then run equivalent experiments for the baseline models** (a standard CNN, a Transformer) on the same synthetic data, collect their performance logs, and calculate the same metrics for them.
        5.  Finally, you will programmatically generate a `Polars` DataFrame or a formatted Markdown table that exactly replicates the structure of **Table IV** from the research paper, populating it with the results for the GIF-DU model and the baselines.

* **Section 4: Qualitative Analysis & Visualization**
    * **Action:** Create visualizations that provide deeper insight into the model's behavior.
    * **Implementation:**
        1.  **Confusion Matrix:** Use `seaborn.heatmap()` to plot the confusion matrix for the test set. This visualizes the types of errors the model is making (e.g., is it confusing planets with eclipsing binaries?).
        2.  **ROC Curve:** Plot the Receiver Operating Characteristic (ROC) curve using `sklearn.metrics.roc_curve` and calculate the Area Under the Curve (AUC). This is a standard way to evaluate the performance of a binary classifier.

---

**Summary of your task:**

1.  Create the analysis notebook `applications/poc_exoplanet/analysis.ipynb`.
2.  Implement the notebook with structured sections for data loading, training visualization, quantitative analysis, and qualitative analysis.
3.  Write the Python code using `polars`, `matplotlib`, and `scikit-learn` to perform all necessary calculations and generate plots.
4.  The primary output of this task is the code that automatically generates the final results table (replicating Table IV from your paper) and the supporting diagnostic plots (loss curves, confusion matrix, ROC curve).
5.  Assume that you can run the main experiment scripts for both the GIF-DU and the baseline models to produce the necessary log files.

Now, following your protocol, please formulate your micro-plan for this task.

**Awaiting approval to proceed.**